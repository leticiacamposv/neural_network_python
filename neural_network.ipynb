{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de ativação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, derivative=False):\n",
    "    return np.ones_like(x) if derivative else x\n",
    "\n",
    "def sigmoid(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = sigmoid(x)\n",
    "        return y*(1 - y)\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x, derivative=False):\n",
    "    if derivative:\n",
    "        y = tanh(x)\n",
    "        return 1 - y**2\n",
    "    return (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "def relu(x, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, derivative=False):\n",
    "    alpha = 0.1\n",
    "    if derivative:\n",
    "        return np.where(x <= 0, alpha, 1)\n",
    "    return np.where(x <= 0, alpha*x, x)\n",
    "\n",
    "def elu(x, derivative=False):\n",
    "    alpha = 1.0\n",
    "    if derivative:\n",
    "        y = elu(x)\n",
    "        return np.where(x <= 0, y + alpha, 1)\n",
    "    return np.where(x <= 0, alpha*(np.exp(x) - 1), x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outras funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, y_oh=None, derivative=False):\n",
    "    if derivative: \n",
    "        y_pred = softmax(x)\n",
    "        y_correct = np.argmax(y_oh, axis=1)\n",
    "        pk = y_pred[range(y_pred.shape[0]), y_correct]\n",
    "        y_pred[range(y_pred.shape[0]), y_correct] = pk*(1.0 - pk)\n",
    "        return y_pred\n",
    "    exp = np.exp(x)\n",
    "    return exp/np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_correct = np.argmax(y_oh, axis=1)\n",
    "    pk = y_pred[range(y_pred.shape[0]), y_correct]\n",
    "    if derivative:\n",
    "        y_pred[range(y_pred.shape[0]), y_correct] = (-1.0/pk)\n",
    "        return y_pred\n",
    "    return np.mean(-np.log(pk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções de custo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return np.where(y_pred > y, 1, -1) / y.shape[0]\n",
    "    return np.mean(np.abs(y - y_pred))\n",
    "\n",
    "def mse(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / y.shape[0]\n",
    "    return 0.5*np.mean((y - y_pred)**2)\n",
    "\n",
    "def binary_cross_entropy(y, y_pred, derivative=False):\n",
    "    if derivative:\n",
    "        return -(y - y_pred) / (y_pred * (1-y_pred) * y.shape[0])\n",
    "    return -np.mean(y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def sigmoid_cross_entropy(y, y_pred, derivative=False):\n",
    "    y_sigmoid = sigmoid(y_pred)\n",
    "    if derivative:\n",
    "        return -(y - y_sigmoid) / y.shape[0]\n",
    "    return -np.mean(y*np.log(y_sigmoid) + (1-y)*np.log(1-y_sigmoid))\n",
    "\n",
    "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
    "    y_softmax = softmax(y_pred)\n",
    "    y_correct = np.argmax(y_oh, axis=1)\n",
    "    pk = y_softmax[range(y_softmax.shape[0]), y_correct]\n",
    "    if derivative:\n",
    "        return -(y_oh - y_softmax)/y_oh.shape[0]\n",
    "    return np.mean(-np.log(pk))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeros(rows,cols):\n",
    "    return np.zeros((rows,cols))\n",
    "\n",
    "def ones(rows,cols):\n",
    "    return np.ones((rows,cols))\n",
    "\n",
    "def random_normal(rows,cols):\n",
    "    return np.random.randn(rows, cols)\n",
    "\n",
    "def random_uniform(rows,cols):\n",
    "    return np.random.rand(rows, cols)\n",
    "\n",
    "def glorot_normal(rows,cols):\n",
    "    std_dev = np.sqrt(2.0 / (rows + cols))\n",
    "    return std_dev * np.random.randn(rows, cols)\n",
    "\n",
    "def glorot_uniform(rows,cols):\n",
    "    limit = np.sqrt(6.0 / (rows + cols))\n",
    "    return 2*limit*np.random.randn(rows, cols)-limit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        weights = [np.where(w<0, -1, w) for w in weights]\n",
    "        return np.array([np.where(w>0, 1,w) for w in weights])\n",
    "    return np.sum([np.sum(np.abs(w)) for w in weights])\n",
    "\n",
    "def l2_regularization(weights, derivative=False):\n",
    "    if derivative:\n",
    "        return weights\n",
    "    return 0.5 * np.sum(weights**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sequential(x, y, batch_size=None):\n",
    "    batch_size = x.shape[0] if batch_size is None else batch_size\n",
    "    n_batches = x.shape[0] // batch_size\n",
    "    \n",
    "    for batch in range(n_batches):\n",
    "        offset = batch_size * batch\n",
    "        x_batch, y_batch = x[offset:offset+batch_size], y[offset:offset+batch_size]\n",
    "        yield (x_batch, y_batch)\n",
    "        \n",
    "def batch_shuffle(x,y,batch_size=None):\n",
    "    shuffle_index = np.random.permutation(range(x.shape[0]))\n",
    "    return batch_sequential(x[shuffle_index], y[shuffle_index], batch_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchnorm_forward(layer, x, is_training=True):\n",
    "    mu = np.mean(x, axis=0) if is_training else layer._pop_mean\n",
    "    var = np.var(x, axis=0) if is_training else layer._pop_var\n",
    "    x_norm = (x - mu) / np.sqrt(var + 1e-8)\n",
    "    out = layer.gamma * x_norm + layer.beta\n",
    "\n",
    "    if is_training:\n",
    "        layer._pop_mean = layer.bn_decay * layer._pop_mean + (1.0-layer.bn_decay)*mu\n",
    "        layer._pop_var = layer.bn_decay * layer._pop_var + (1.0-layer.bn_decay)*var\n",
    "        layer._bn_cache = (x, x_norm, mu, var)\n",
    "    return out\n",
    "\n",
    "def batchnorm_backward(layer, dactivation):\n",
    "    x, x_norm, mu, var = layer._bn_cache\n",
    "    m = layer._activ_inp.shape[0]\n",
    "    x_mu = x - mu\n",
    "    std_inv = 1. / np.sqrt(var + 1e-8)\n",
    "\n",
    "    dx_norm = dactivation * layer.gamma\n",
    "    dvar = np.sum(dx_norm * x_mu, axis=0) * -0.5 * (std_inv**3)\n",
    "    dmu = np.sum(dx_norm * -std_inv, axis=0) + dvar * np.mean(-2.0 * x_mu, axis=0)\n",
    "\n",
    "    dx = (dx_norm * std_inv) + (dvar * 2.0 * x_mu / m) + (dmu / m)\n",
    "    layer._dgamma = np.sum(dactivation * x_norm, axis=0)\n",
    "    layer._dbeta = np.sum(dactivation, axis=0)\n",
    "    return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Rate Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate\n",
    "\n",
    "def time_based_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return 1.0 / (1+decay_rate*epoch)\n",
    "\n",
    "def exponential_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** epoch\n",
    "\n",
    "def staircase_decay(learning_rate, epoch, decay_rate, decay_steps=1):\n",
    "    return learning_rate * decay_rate ** (epoch//decay_steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute_approx_grads(nn, x, y, eps=1e-4):\n",
    "    approx_grads = []\n",
    "    feed_forward = lambda inp: nn._NeuralNetwork__feedforward(inp)\n",
    "\n",
    "    for layer in nn.layers:\n",
    "        assert(layer.dropout_prob == 0), \"O Gradient Checking não pode ser aplicado em redes com DROPOUT\"\n",
    "\n",
    "        w_ori = layer.weights.copy()\n",
    "        w_ravel = w_ori.ravel()\n",
    "        w_shape = w_ori.shape\n",
    "\n",
    "        for i in range(w_ravel.size):\n",
    "            w_plus = w_ravel.copy()\n",
    "            w_plus[i] += eps\n",
    "            layer.weights = w_plus.reshape(w_shape)\n",
    "            J_plus = nn.cost_func(y, feed_forward(x)) + (1.0 / y.shape[0]) * layer.reg_strength + layer.reg_func(layer.weights)\n",
    "\n",
    "            w_minus = w_ravel.copy()\n",
    "            w_minus[i] -= eps\n",
    "            layer.weights = w_minus.reshape(w_shape)\n",
    "            J_minus = nn.cost_func(y, feed_forward(x))\n",
    "            approx_grads.append((J_plus - J_minus) / (2.0*eps))\n",
    "        layer.weights = w_ori\n",
    "\n",
    "    return approx_grads\n",
    "\n",
    "def gradient_checking(nn, x, y, eps=1e-4, verbose=False, verbose_precision=5):\n",
    "    from copy import deepcopy\n",
    "    nn_copy = deepcopy(nn)\n",
    "\n",
    "    nn.fit(x, y, epochs=0)\n",
    "    grads = np.concatenate([layer._dweights.ravel() for layer in nn.layers])\n",
    "\n",
    "    approx_grads = __compute_approx_grads(nn_copy, x, y, eps)\n",
    "\n",
    "    is_close = np.allclose(grads, approx_grads)\n",
    "    print(\"{}\".format(\"\\033[92mGRADIENTS OK\" if is_close else \"\\033[91mGRADIENTS FAIL\"))\n",
    "\n",
    "    norm_num = np.linalg.norm(grads - approx_grads)\n",
    "    norm_den = np.linalg.norm(grads) + np.linalg.norm(approx_grads)\n",
    "    error = norm_num / norm_den\n",
    "    print(\"Relative error:\", error)\n",
    "\n",
    "    if verbose:\n",
    "        np.set_printoptions(precision=verbose_precision, linewidth=200, suppress=True)\n",
    "        print(\"Gradientes:\", grads)\n",
    "        print(\"Aproximado:\", np.array(approx_grads))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    def __init__(self, input_dim, output_dim, activation=linear, weights_initializer=random_normal, biases_initializer=ones, \n",
    "                 dropout_prob = 0.0, reg_func=l2_regularization, reg_strength=0.0, batch_norm=False, bn_decay=0.9, is_trainable=True):\n",
    "        self.input = None\n",
    "        self.weights = weights_initializer(output_dim, input_dim)\n",
    "        self.biases = biases_initializer(1, output_dim)\n",
    "        self.activation = activation\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.reg_func = reg_func\n",
    "        self.reg_strength = reg_strength\n",
    "        self.batch_norm = batch_norm\n",
    "        self.bn_decay = bn_decay\n",
    "        self.gamma, self.beta = ones(1, output_dim), zeros(1, output_dim)\n",
    "        self.is_trainable = is_trainable\n",
    "\n",
    "        self._activ_inp, self._activ_out = None, None\n",
    "        self._dweights, self._dbiases, self._prev_dweights = None, None, 0.0\n",
    "        self._dropout_mask = None\n",
    "        self._dgamma, self._dbeta = None, None\n",
    "        self._pop_mean, self._pop_var = zeros(1, output_dim), zeros(1, output_dim)\n",
    "        self._bn_cache = None\n",
    "        \n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, cost_func=mse, learning_rate=1e-3, lr_decay_method=none_decay, lr_decay_rate=0.0, \n",
    "                 lr_decay_steps=1, momentum=0.0, patience=np.inf):\n",
    "        self.layers = []\n",
    "        self.cost_func = cost_func\n",
    "        self.learning_rate = self.lr_initial = learning_rate\n",
    "        self.lr_decay_method = lr_decay_method\n",
    "        self.lr_decay_rate = lr_decay_rate\n",
    "        self.lr_decay_steps = lr_decay_steps\n",
    "        self.momentum = momentum\n",
    "        self.patience, self.waiting = patience, 0\n",
    "        self._best_model, self._best_loss = self.layers, np.inf\n",
    "        \n",
    "    def fit(self, x_train, y_train, x_val=None, y_val=None, epochs=100, verbose=10, batch_gen=batch_sequential, batch_size=None):\n",
    "        x_val, y_val = (x_train, y_train) if (x_val is None or y_val is None) else (x_val, y_val)\n",
    "        for epoch in range(epochs+1):\n",
    "            self.learning_rate = self.lr_decay_method(self.lr_initial, epoch, self.lr_decay_rate, self.lr_decay_steps)\n",
    "            for x_batch, y_batch in batch_gen(x_train, y_train, batch_size):\n",
    "                y_pred = self.__feedforward(x_batch)\n",
    "                self.__backprop(y_batch, y_pred)\n",
    "             \n",
    "            #Validation\n",
    "            loss_val = self.cost_func(y_val, self.predict(x_val))\n",
    "            if loss_val < self._best_loss:\n",
    "                self._best_model, self._best_loss = self.layers, loss_val\n",
    "                self.waiting = 0\n",
    "            else:\n",
    "                self.waiting += 1\n",
    "                if self.waiting >= self.patience:\n",
    "                    self.layers = self._best_model\n",
    "                    return\n",
    "                \n",
    "            # Função de custo\n",
    "            if epoch % verbose == 0:\n",
    "                loss_train = self.cost_func(y_train, self.predict(x_train))\n",
    "                loss_reg = (1.0 / y_train.shape[0]) * np.sum([layer.reg_strength * layer.reg_func(layer.weights) for layer in self.layers])\n",
    "                print(\"epoch: {0:=4}/{1} loss_train: {2:.8f} + {3:.8f} = {4:.8f} loss_val = {5:.8f}\".format(epoch, epochs, loss_train, loss_reg, loss_train + loss_reg, loss_val))\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.__feedforward(x, is_training=False)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        pkl.dump(self, open(file_path, 'wb)'), -1)\n",
    "\n",
    "    def load(file_path):\n",
    "        return pkl.load(open(file_path, 'rb)'))\n",
    "\n",
    "    def __feedforward(self, x, is_training=True):\n",
    "        self.layers[0].input = x\n",
    "        for current_layer, next_layer in zip(self.layers, self.layers[1:] + [Layer(0, 0)]):\n",
    "            y = np.dot(current_layer.input, current_layer.weights.T) + current_layer.biases\n",
    "            y = batchnorm_forward(current_layer, y, is_training) if current_layer.batch_norm else y\n",
    "            current_layer._dropout_mask = np.random.binomial(1, 1.0-current_layer.dropout_prob, y.shape) / (1.0-current_layer.dropout_prob)\n",
    "            current_layer._activ_inp = y\n",
    "            current_layer._activ_out = current_layer.activation(y) * (current_layer._dropout_mask if is_training else 1.0)\n",
    "            next_layer.input = current_layer._activ_out\n",
    "        return self.layers[-1]._activ_out\n",
    "\n",
    "    def __backprop(self, y, y_pred):\n",
    "        # Atualização dos pesos\n",
    "        last_delta = self.cost_func(y, y_pred, derivative=True)\n",
    "        for layer in reversed(self.layers):\n",
    "            dactivation = layer.activation(layer._activ_inp, derivative=True) * last_delta * layer._dropout_mask\n",
    "            dactivation = batchnorm_backward(layer, dactivation) if layer.batch_norm else dactivation\n",
    "            last_delta = np.dot(dactivation, layer.weights)\n",
    "            \n",
    "            layer._dweights = np.dot(dactivation.T, layer.input)\n",
    "            layer._dbiases = 1.0*dactivation.sum(axis=0, keepdims=True)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            if layer.is_trainable:\n",
    "                layer._dweights = layer._dweights + (1.0/y.shape[0]) * layer.reg_strength * layer.reg_func(layer.weights, derivative=True)\n",
    "                layer._prev_dweights = - self.learning_rate*layer._dweights + self.momentum*layer._prev_dweights\n",
    "                layer.weights = layer.weights + layer._prev_dweights\n",
    "                layer.biases = layer.biases - self.learning_rate*layer._dbiases\n",
    "                if layer.batch_norm:\n",
    "                    layer.gamma = layer.gamma - self.learning_rate*layer._dgamma\n",
    "                    layer.beta = layer.beta - self.learning_rate*layer._dbeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.2 (tags/v3.9.2:1a79785, Feb 19 2021, 13:44:55) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a292b490e86f4c929d1419110283d173ef6bc08b4a03640546d532aca137e54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
